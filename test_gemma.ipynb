from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "google/gemma-2-2b"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True
)

print("Gemma model and tokenizer loaded successfully!")
